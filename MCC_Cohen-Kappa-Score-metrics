# from sklearn.metrics import matthews_corrcoef

# # Compute Matthews Correlation Coefficient (MCC) for both models
# mcc_baseline = matthews_corrcoef(y_true_baseline, y_pred_baseline)
# mcc_okn_c = matthews_corrcoef(y_true_okn_c, y_pred_okn_c)

# # Display results
# mcc_baseline, mcc_okn_c

# ==========================

from sklearn.metrics import cohen_kappa_score, matthews_corrcoef
import numpy as np

def expand_conf_matrix(conf_matrix):
    """
    Convert confusion matrix counts into classification labels for metrics calculation.
    """
    tn, fp, fn, tp = conf_matrix.ravel()
    y_true = np.array([0] * (tn + fp) + [1] * (fn + tp))  # Ground truth labels
    y_pred = np.array([0] * tn + [1] * fp + [0] * fn + [1] * tp)  # Model predictions
    return y_true, y_pred

def compute_metrics(conf_matrix):
    """
    Compute Cohen's Kappa and MCC for a given confusion matrix.
    """
    y_true, y_pred = expand_conf_matrix(conf_matrix)
    kappa = cohen_kappa_score(y_true, y_pred)
    mcc = matthews_corrcoef(y_true, y_pred)
    return kappa, mcc

# Define confusion matrices for both models
baseline_conf_matrix = np.array([[147, 7], [49, 271]])  # TN, FP, FN, TP
okn_c_conf_matrix = np.array([[148, 6], [36, 284]])  # TN, FP, FN, TP

# Compute metrics for both models
kappa_baseline, mcc_baseline = compute_metrics(baseline_conf_matrix)
kappa_okn_c, mcc_okn_c = compute_metrics(okn_c_conf_matrix)

# Display results
print(f"Baseline Model - Cohen's Kappa: {kappa_baseline:.3f}, MCC: {mcc_baseline:.3f}")
print(f"OKN-C Model - Cohen's Kappa: {kappa_okn_c:.3f}, MCC: {mcc_okn_c:.3f}")

# Determine the better model based on MCC
better_model = "OKN-C Model" if mcc_okn_c > mcc_baseline else "Baseline Model"
print(f"The better model based on MCC is: {better_model}")

